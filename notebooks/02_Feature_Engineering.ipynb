{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Based on EDA findings:\n",
    "- Time features\n",
    "- Amount transforms \n",
    "- Category flags\n",
    "- Customer age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data/interim')\n",
    "PROCESSED_DIR = Path('../data/processed')\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,852,394 rows\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(DATA_DIR / 'transactions_merged.csv')\n",
    "\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Night transactions: 33.2%\n"
     ]
    }
   ],
   "source": [
    "df['hour'] = df['trans_date_trans_time'].dt.hour\n",
    "df['day_of_week'] = df['trans_date_trans_time'].dt.dayofweek\n",
    "df['month'] = df['trans_date_trans_time'].dt.month\n",
    "\n",
    "# Night: 22:00 - 06:00 (high fraud hours from EDA - 5x higher fraud rate)\n",
    "df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
    "print(f\"Night transactions: {df['is_night'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud rate by is_night:\n",
      "is_night\n",
      "0    0.104047\n",
      "1    1.358999\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "print(\"Fraud rate by is_night:\")\n",
    "print(df.groupby('is_night')['is_fraud'].mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "Night flag captures the high-fraud pattern from EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Amount Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount range: $1.00 - $28948.90\n",
      "Log amount range: 0.69 - 10.27\n"
     ]
    }
   ],
   "source": [
    "# Log transform\n",
    "df['log_amt'] = np.log1p(df['amt'])\n",
    "\n",
    "print(f\"Amount range: ${df['amt'].min():.2f} - ${df['amt'].max():.2f}\")\n",
    "print(f\"Log amount range: {df['log_amt'].min():.2f} - {df['log_amt'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Category Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud rate by is_online:\n",
      "is_online\n",
      "0    0.390038\n",
      "1    1.212804\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Online flag (categories ending with _net)\n",
    "df['is_online'] = df['category'].str.endswith('_net').astype(int)\n",
    "\n",
    "print(\"Fraud rate by is_online:\")\n",
    "print(df.groupby('is_online')['is_fraud'].mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total merchants: 693\n",
      "Median merchant transactions: 2,663\n",
      "\n",
      "Top 10 merchants by fraud rate (with >100 transactions):\n",
      "                                            fraud_rate  count\n",
      "merchant                                                     \n",
      "fraud_Kozey-Boehm                             0.021755   2758\n",
      "fraud_Herman, Treutel and Dickens             0.020321   1870\n",
      "fraud_Terry-Huel                              0.019553   2864\n",
      "fraud_Kerluke-Abshire                         0.018975   2635\n",
      "fraud_Mosciski, Ziemann and Farrell           0.018788   2821\n",
      "fraud_Schmeler, Bashirian and Price           0.018651   2788\n",
      "fraud_Kuhic LLC                               0.018649   2842\n",
      "fraud_Jast Ltd                                0.018498   2757\n",
      "fraud_Langworth, Boehm and Gulgowski          0.018459   2817\n",
      "fraud_Romaguera, Cruickshank and Greenholt    0.018432   2767\n",
      "\n",
      "Merchant risk distribution:\n",
      "merchant_risk\n",
      "medium    1098250\n",
      "high       381034\n",
      "low        373110\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merchant risk - similar approach to state risk\n",
    "merchant_stats = df.groupby('merchant').agg(\n",
    "    fraud_rate=('is_fraud', 'mean'),\n",
    "    count=('is_fraud', 'count')\n",
    ")\n",
    "\n",
    "print(f\"Total merchants: {len(merchant_stats)}\")\n",
    "median_merch_count = merchant_stats['count'].median()\n",
    "print(f\"Median merchant transactions: {median_merch_count:,.0f}\")\n",
    "\n",
    "# Top fraud merchants\n",
    "print(\"\\nTop 10 merchants by fraud rate (with >100 transactions):\")\n",
    "print(merchant_stats[merchant_stats['count'] > 100].sort_values('fraud_rate', ascending=False).head(10))\n",
    "\n",
    "# Merchants below median -> 'medium', others -> quantile\n",
    "low_count_mask = merchant_stats['count'] < median_merch_count\n",
    "reliable_merchants = merchant_stats[~low_count_mask]\n",
    "merchant_risk_reliable = pd.qcut(reliable_merchants['fraud_rate'], q=3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "merchant_risk_map = merchant_risk_reliable.to_dict()\n",
    "for merch in merchant_stats[low_count_mask].index:\n",
    "    merchant_risk_map[merch] = 'medium'\n",
    "\n",
    "df['merchant_risk'] = df['merchant'].map(merchant_risk_map)\n",
    "\n",
    "print(f\"\\nMerchant risk distribution:\")\n",
    "print(df['merchant_risk'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Customer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age range: 13 - 96\n"
     ]
    }
   ],
   "source": [
    "# Age at transaction\n",
    "df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days // 365\n",
    "\n",
    "print(f\"Age range: {df['age'].min()} - {df['age'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geographic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance (not very predictive from EDA, but include)\n",
    "df['distance'] = np.sqrt(\n",
    "    (df['lat'] - df['merch_lat'])**2 + \n",
    "    (df['long'] - df['merch_long'])**2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Risk Labels (based on internal fraud rates)\n",
    "\n",
    "Create risk categories using fraud rates from our data. Could also use external crime data but internal rates are more relevant for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category stats:\n",
      "                fraud_rate   count\n",
      "category                          \n",
      "shopping_net      0.015927  139322\n",
      "misc_net          0.013039   90654\n",
      "grocery_pos       0.012645  176191\n",
      "shopping_pos      0.006344  166463\n",
      "gas_transport     0.004106  188029\n",
      "misc_pos          0.002819  114229\n",
      "grocery_net       0.002697   64878\n",
      "travel            0.002692   57956\n",
      "personal_care     0.002229  130085\n",
      "entertainment     0.002177  134118\n",
      "kids_pets         0.001880  161727\n",
      "food_dining       0.001568  130729\n",
      "home              0.001510  175460\n",
      "health_fitness    0.001510  122553\n",
      "\n",
      "Category risk distribution:\n",
      "category_risk\n",
      "high      760659\n",
      "low       724587\n",
      "medium    367148\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Category risk labels using quantiles (not hardcoded thresholds)\n",
    "category_stats = df.groupby('category').agg(\n",
    "    fraud_rate=('is_fraud', 'mean'),\n",
    "    count=('is_fraud', 'count')\n",
    ")\n",
    "print(\"Category stats:\")\n",
    "print(category_stats.sort_values('fraud_rate', ascending=False))\n",
    "\n",
    "# Use quantiles for risk levels\n",
    "category_risk = pd.qcut(category_stats['fraud_rate'], q=3, labels=['low', 'medium', 'high'])\n",
    "df['category_risk'] = df['category'].map(category_risk)\n",
    "\n",
    "print(\"\\nCategory risk distribution:\")\n",
    "print(df['category_risk'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median state transactions: 32,939\n",
      "Mean state transactions: 36,321  (higher due to big states)\n",
      "\n",
      "States with < median transactions: 25\n",
      "       fraud_rate  count  fraud_count\n",
      "state                                \n",
      "DE       1.000000      9            9\n",
      "RI       0.020134    745           15\n",
      "AK       0.016875   2963           50\n",
      "HI       0.004385   3649           16\n",
      "DC       0.006043   5130           31\n",
      "ID       0.004107   8035           33\n",
      "NV       0.005833   8058           47\n",
      "CT       0.005101  10979           56\n",
      "NH       0.006737  11727           79\n",
      "UT       0.003972  15357           61\n",
      "\n",
      "State risk distribution:\n",
      "state_risk\n",
      "medium    844618\n",
      "low       554419\n",
      "high      453357\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# State risk labels - handle low transaction counts\n",
    "state_stats = df.groupby('state').agg(\n",
    "    fraud_rate=('is_fraud', 'mean'),\n",
    "    count=('is_fraud', 'count'),\n",
    "    fraud_count=('is_fraud', 'sum')\n",
    ")\n",
    "\n",
    "# Use MEDIAN (not mean) because:\n",
    "# - Mean is sensitive to outliers (big states like CA, TX, NY pull it up)\n",
    "# - Median gives the actual middle value - more robust\n",
    "median_count = state_stats['count'].median()\n",
    "mean_count = state_stats['count'].mean()\n",
    "print(f\"Median state transactions: {median_count:,.0f}\")\n",
    "print(f\"Mean state transactions: {mean_count:,.0f}  (higher due to big states)\")\n",
    "\n",
    "# States below median count -> set to 'medium' (unreliable fraud rate)\n",
    "low_count_mask = state_stats['count'] < median_count\n",
    "print(f\"\\nStates with < median transactions: {low_count_mask.sum()}\")\n",
    "print(state_stats[low_count_mask].sort_values('count').head(10))\n",
    "\n",
    "# For states with enough data, use quantiles\n",
    "reliable_states = state_stats[~low_count_mask]\n",
    "state_risk_reliable = pd.qcut(reliable_states['fraud_rate'], q=3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "# Create full mapping: low-count states -> 'medium', others -> quantile-based\n",
    "state_risk_map = state_risk_reliable.to_dict()\n",
    "for state in state_stats[low_count_mask].index:\n",
    "    state_risk_map[state] = 'medium'  # Unreliable -> neutral\n",
    "\n",
    "df['state_risk'] = df['state'].map(state_risk_map)\n",
    "\n",
    "print(f\"\\nState risk distribution:\")\n",
    "print(df['state_risk'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud rate by category_risk:\n",
      "category_risk\n",
      "low       0.172650\n",
      "medium    0.256845\n",
      "high      0.980334\n",
      "Name: is_fraud, dtype: float64\n",
      "\n",
      "Fraud rate by state_risk:\n",
      "state_risk\n",
      "high      0.591807\n",
      "low       0.456334\n",
      "medium    0.525445\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_272938/509162737.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  print(df.groupby('category_risk')['is_fraud'].mean() * 100)\n"
     ]
    }
   ],
   "source": [
    "# Verify risk labels capture fraud pattern\n",
    "print(\"Fraud rate by category_risk:\")\n",
    "print(df.groupby('category_risk')['is_fraud'].mean() * 100)\n",
    "\n",
    "print(\"\\nFraud rate by state_risk:\")\n",
    "print(df.groupby('state_risk')['is_fraud'].mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Encoding\n",
    "\n",
    "- One-hot encode low cardinality: gender, category_risk, state_risk\n",
    "- Label encode or drop high cardinality: category (14), state (51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded columns added:\n",
      "['cat_risk_medium', 'cat_risk_high', 'state_risk_low', 'state_risk_medium', 'merch_risk_low', 'merch_risk_medium', 'gender_M']\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode risk labels and gender\n",
    "df_encoded = pd.get_dummies(df, columns=['category_risk', 'state_risk', 'merchant_risk', 'gender'], \n",
    "                            prefix=['cat_risk', 'state_risk', 'merch_risk', 'gender'],\n",
    "                            drop_first=True)\n",
    "\n",
    "print(\"Encoded columns added:\")\n",
    "print([c for c in df_encoded.columns if 'risk' in c or 'gender' in c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (1852394, 17)\n",
      "\n",
      "Features (16):\n",
      "  - amt\n",
      "  - log_amt\n",
      "  - city_pop\n",
      "  - age\n",
      "  - distance\n",
      "  - hour\n",
      "  - day_of_week\n",
      "  - month\n",
      "  - is_night\n",
      "  - is_online\n",
      "  - cat_risk_medium\n",
      "  - state_risk_low\n",
      "  - state_risk_medium\n",
      "  - merch_risk_low\n",
      "  - merch_risk_medium\n",
      "  - gender_M\n"
     ]
    }
   ],
   "source": [
    "# Final feature columns\n",
    "feature_cols = [\n",
    "    # Numerical\n",
    "    'amt', 'log_amt', 'city_pop', 'age', 'distance',\n",
    "    'hour', 'day_of_week', 'month',\n",
    "    \n",
    "    # Binary flags (based on EDA findings)\n",
    "    'is_night',    # 5x higher fraud rate at night\n",
    "    'is_online',   # 3x higher fraud rate online\n",
    "    \n",
    "    # One-hot encoded risk labels\n",
    "    'cat_risk_low', 'cat_risk_medium',\n",
    "    'state_risk_low', 'state_risk_medium',\n",
    "    'merch_risk_low', 'merch_risk_medium',\n",
    "    'gender_M'\n",
    "]\n",
    "\n",
    "# Filter existing columns\n",
    "feature_cols = [c for c in feature_cols if c in df_encoded.columns]\n",
    "target = 'is_fraud'\n",
    "\n",
    "df_final = df_encoded[feature_cols + [target]].copy()\n",
    "print(f\"Final shape: {df_final.shape}\")\n",
    "print(f\"\\nFeatures ({len(feature_cols)}):\")\n",
    "for col in feature_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why these columns were excluded:\n",
    "\n",
    "| Column | Reason |\n",
    "|--------|--------|\n",
    "| `trans_date_trans_time` | Extracted `hour`, `month`, `day_of_week` from it |\n",
    "| `cc_num` | Identifier, not a feature |\n",
    "| `merchant` | Replaced with `merchant_risk` (693 unique -> 3 risk levels) |\n",
    "| `first`, `last` | Names are not predictive |\n",
    "| `street`, `city`, `zip` | High cardinality, used `state_risk` instead |\n",
    "| `lat`, `long`, `merch_lat`, `merch_long` | Created `distance` feature instead |\n",
    "| `dob` | Created `age` feature instead |\n",
    "| `trans_num` | Transaction ID |\n",
    "| `unix_time` | Same info as datetime |\n",
    "| `category` | Replaced with `is_online` + `category_risk` |\n",
    "| `state` | Replaced with `state_risk` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../data/processed/features.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "df_final.to_parquet(PROCESSED_DIR / 'features.parquet', index=False)\n",
    "print(f\"Saved to {PROCESSED_DIR / 'features.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Created 16 features from 23 original columns:**\n",
    "\n",
    "| Feature Type | Features | Notes |\n",
    "|-------------|----------|-------|\n",
    "| Numerical | `amt`, `log_amt`, `city_pop`, `age`, `distance` | Log transform for skewed amount |\n",
    "| Time | `hour`, `day_of_week`, `month` | Extracted from timestamp |\n",
    "| Binary | `is_night`, `is_online` | Based on EDA findings |\n",
    "| Risk (one-hot) | `cat_risk_*`, `state_risk_*`, `merch_risk_*` | Quantile-based risk labels |\n",
    "| Demographics | `gender_M` | One-hot encoded |\n",
    "\n",
    "**Key decisions:**\n",
    "- Used quantiles (not hardcoded thresholds) for risk labels\n",
    "- Low transaction count states/merchants set to 'medium' (unreliable statistics)\n",
    "- Dropped high-cardinality columns in favor of risk aggregations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-detection (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
